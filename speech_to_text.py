from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import sounddevice as sd
from scipy.io import wavfile
import keyboard
import winsound
import time
import torch
import numpy as np
import os

# –≤–∞—Ä–∏–∞–Ω—Ç –≥–¥–µ –≤—Å–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper...")
device = "cuda" if torch.cuda.is_available() else "cpu"

processor = AutoProcessor.from_pretrained("openai/whisper-large-v3-turbo")
model = AutoModelForSpeechSeq2Seq.from_pretrained("openai/whisper-large-v3-turbo").to(device)

print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {device.upper()}")

is_recording = False
audio_data = []
SAMPLE_RATE = 16000

def play_start_beep():
    # winsound.Beep(1500, 200)
    # time.sleep(0.1)
    winsound.Beep(1500, 200)

def play_stop_beep():
    winsound.Beep(400, 400)

def audio_callback(indata, frames, time, status):
    if is_recording:
        audio_data.append(indata.copy())

def process_audio(audio_file):
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ —Ñ–∞–π–ª
    sample_rate, audio_data = wavfile.read(audio_file)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float32 –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    audio_data = audio_data.astype(np.float32) / 32768.0
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    inputs = processor(
        audio_data, 
        sampling_rate=sample_rate, 
        return_tensors="pt",
        padding=True
    )
    
    input_features = inputs.input_features.to(device)
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è –≤—Ä—É—á–Ω—É—é
    attention_mask = torch.ones_like(input_features[:, :, 0], dtype=torch.long).to(device)
    
    forced_decoder_ids = processor.get_decoder_prompt_ids(language="ru", task="transcribe")
    
    predicted_ids = model.generate(
        input_features,
        attention_mask=attention_mask,
        forced_decoder_ids=forced_decoder_ids,
        max_length=448,
    )
    
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

print("\n–ù–∞–∂–º–∏—Ç–µ F7 –¥–ª—è –Ω–∞—á–∞–ª–∞/–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø–∏—Å–∏")
print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")

# –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
with sd.InputStream(callback=audio_callback, channels=1, samplerate=SAMPLE_RATE, dtype=np.int16):
    while True:
        try:
            if keyboard.is_pressed('f7'):
                is_recording = not is_recording
                if is_recording:
                    print("\nüé§ –ó–ê–ü–ò–°–¨ –ù–ê–ß–ê–¢–ê")
                    play_start_beep()
                    audio_data.clear()  # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä –ø–µ—Ä–µ–¥ –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å—å—é
                else:
                    print("\n‚èπÔ∏è –ó–ê–ü–ò–°–¨ –û–°–¢–ê–ù–û–í–õ–ï–ù–ê")
                    play_stop_beep()
                    
                    if len(audio_data) > 0:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ
                        recorded_audio = np.concatenate(audio_data, axis=0)
                        wavfile.write('recorded_audio.wav', SAMPLE_RATE, recorded_audio)
                        print("\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏...")
                        
                        # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ä–µ—á—å
                        transcription = process_audio('recorded_audio.wav')
                        print(f"\n–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {transcription}")
                
                # –ñ–¥–µ–º, –ø–æ–∫–∞ –∫–ª–∞–≤–∏—à–∞ –±—É–¥–µ—Ç –æ—Ç–ø—É—â–µ–Ω–∞
                while keyboard.is_pressed('f7'):
                    time.sleep(0.1)
                time.sleep(0.3)
                
        except KeyboardInterrupt:
            print("\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            break
        
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞: {e}") 